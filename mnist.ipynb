{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition\n",
    "\n",
    "\n",
    "\n",
    "The ReLU function takes a single input \n",
    "z and produces an output based on the rule:\n",
    ":\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "or as a piecewise function:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) =\n",
    "\\begin{cases}\n",
    "z & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Step-by-Step Operation\n",
    "1. **Input**: The function receives an input \\( z \\), which can be any real number.\n",
    "2. **Condition Check**: It checks whether \\( z \\) is greater than zero.\n",
    "- If \\( z > 0 \\), then the function outputs \\( z \\) itself.\n",
    "- If \\( z \\leq 0 \\) (including zero and negative values), the output is \\( 0 \\).\n",
    "### Graphical Representation\n",
    "Graphically, the ReLU function can be visualized as a line that passes through the origin (0,0) and slopes upward with a slope of 1 for all positive values of \\( z \\). For all non-positive values, the line stays at 0 on the y-axis.\n",
    "### Why Use ReLU?\n",
    "- **Non-linearity**: ReLU introduces non-linearity into the model, which is essential for learning complex patterns in data.\n",
    "- **Computationally Efficient**: It is simple and computationally efficient because it involves basic thresholding at zero.\n",
    "- **Reduces Vanishing Gradient Problem**: During backpropagation, gradients for positive inputs are 1, and for negative inputs are 0, which helps in mitigating the vanishing gradient problem common with other activation functions like sigmoid or tanh.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **softmax function** transforms a vector \\( \\mathbf{z} \\) of real values into a probability distribution over predicted output classes. It's defined by the formula:\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "where:\n",
    "\n",
    "\n",
    "$$\\sigma(\\mathbf{z})_i$$ is the probability of the $$( i )$$-th class, $$( e^{z_i})$$ is the exponential of the $$( i )$$-th input, and $$( \\sum_{k=1}^K e^{z_k} )$$ is the sum of exponentials of all inputs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Understanding Softmax\n",
    "\n",
    "The **softmax function** transforms a list of numbers into probabilities. Here’s how it works:\n",
    "\n",
    "1. **Make Numbers Positive and Exaggerate Differences**: It first calculates the exponential (`e^number`) of each number. This step ensures all numbers are positive and larger numbers stand out more.\n",
    "\n",
    "    Making Numbers Positive and Exaggerating Differences\n",
    "Exponential Function Basics\n",
    "The exponential function $$e^{z_i} \\text{ for each } i$$ is a mathematical function that grows faster as the value of \n",
    "x increases. Here are some key properties of the exponential function that are relevant in the context of the softmax function:\n",
    "\n",
    "Positive Output: For any real number \n",
    "\n",
    "x, \n",
    "\n",
    "\n",
    "e \n",
    "x\n",
    "  is always positive. This property is crucial because it ensures that there are no negative probabilities, which would be nonsensical in the context of probability distributions.\n",
    "\n",
    "Amplifying Differences: The exponential function is an increasing function, which means larger input values will result in exponentially larger output values. This characteristic is particularly useful in highlighting the differences between numbers. Small differences in the input logits can become significant differences in the exponentiated values.\n",
    "\n",
    "Application in Softmax\n",
    "Let's consider a vector of logits, $$\\mathbf{z} = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{bmatrix}$$, representing the raw scores from a model for three classes. Here's what happens when we apply the exponential function:\n",
    "\n",
    "Before Exponentiation: \n",
    "\n",
    "Suppose $$z_1 = 1, \\quad z_2 = 2, \\quad z_3 = 3$$ The differences between each are just 1 unit.\n",
    "\n",
    "After Exponentiation:\n",
    "\n",
    "$$e^{z_1} = e^1 \\approx 2.718, \\quad e^{z_2} = e^2 \\approx 7.389, \\quad e^{z_3} = e^3 \\approx 20.086$$\n",
    "\n",
    "\n",
    "\n",
    "As you can see, after applying the exponential function:\n",
    "\n",
    "All Outputs Are Positive: There are no negative values, fitting the requirement for probabilities.\n",
    "Differences Are Exaggerated: The difference between $$\\(e^{z_1}\\)$$ and $$(e^{z_2})$$ is about 4.671, and between $$(e^{z_2})$$ and $$(e^{z_3})$$ is about 12.697.\n",
    ", compared to the original differences of 1. This exaggeration helps in clearly distinguishing between the logits, especially in scenarios where one class is significantly more likely than the others.\n",
    "Mathematical and Practical Importance\n",
    "This transformation has both mathematical and practical implications:\n",
    "\n",
    "Avoiding Numerical Underflow: By transforming all scores into positive numbers, it guards against issues related to numerical underflow in computers, where very small numbers (close to zero) might be rounded down to zero.\n",
    "Focus on Higher Scores: In classification tasks, you often want the class with the highest logit to have a significantly higher probability after normalization. The exponential function ensures that if one logit is slightly larger than another, its exponential will be much larger, heavily influencing the probability distribution.\n",
    "This step is foundational before moving to normalization, where these exponentiated values are converted into a proper probability distribution that sums to 1, which is covered in the subsequent steps of the softmax function. This ensures that the output of the softmax function reflects both the relative differences and the absolute magnitudes of the input logits in a probabilistically meaningful way.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Add Them Up**: Then, it adds up all these exponentials. This total will help in scaling the numbers down in the next step.\n",
    "\n",
    "3. **Turn Into Probabilities**: Each number’s exponential is then divided by the total from the previous step. This makes each number a probability, with all probabilities adding up to 1.\n",
    "\n",
    "This process is mathematically represented as:\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "Where each $$( \\sigma(\\mathbf{z})_i )$$ is the probability corresponding to each original number after applying the softmax.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
