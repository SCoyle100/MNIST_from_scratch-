{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "Y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the normalization (X-train / 255), each element in the training set matrix will be divided by 255.  \n",
    "\n",
    "Normalization by dividing each pixel value by 255 scales these values to be between 0 and 1. This scaling is common practice in image processing and machine learning, particularly when working with image data, as it helps with the convergence of learning algorithms by ensuring that input features (in this case, pixel values) are on a similar scale.\n",
    "\n",
    "\n",
    "Some sample values (elements from the X_train matrix): 7, 186, 252, 227, 184, 191, 252, 252, 252, 252, 253, 240, 50\n",
    "\n",
    "After normalization: Each of these values will be divided by 255.\n",
    "For example:\n",
    "\n",
    "$$\n",
    "\\[\n",
    "\\frac{7}{255} \\approx 0.027\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\frac{186}{255} \\approx 0.729\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\frac{252}{255} \\approx 0.988\n",
    "\\]\n",
    "$$\n",
    "\n",
    "\n",
    "And so on for the rest of the values. This transformation results in a dataset where all pixel values lie in the range [0, 1], which is easier for many machine learning models to handle efficiently. This normalized dataset would then be used for training your neural network, helping the model learn and make predictions more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition\n",
    "\n",
    "\n",
    "\n",
    "The ReLU function takes a single input \n",
    "z and produces an output based on the rule:\n",
    ":\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "or as a piecewise function:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) =\n",
    "\\begin{cases}\n",
    "z & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Step-by-Step Operation\n",
    "1. **Input**: The function receives an input \\( z \\), which can be any real number.\n",
    "2. **Condition Check**: It checks whether \\( z \\) is greater than zero.\n",
    "- If \\( z > 0 \\), then the function outputs \\( z \\) itself.\n",
    "- If \\( z \\leq 0 \\) (including zero and negative values), the output is \\( 0 \\).\n",
    "### Graphical Representation\n",
    "Graphically, the ReLU function can be visualized as a line that passes through the origin (0,0) and slopes upward with a slope of 1 for all positive values of \\( z \\). For all non-positive values, the line stays at 0 on the y-axis.\n",
    "### Why Use ReLU?\n",
    "- **Non-linearity**: ReLU introduces non-linearity into the model, which is essential for learning complex patterns in data.\n",
    "- **Computationally Efficient**: It is simple and computationally efficient because it involves basic thresholding at zero.\n",
    "- **Reduces Vanishing Gradient Problem**: During backpropagation, gradients for positive inputs are 1, and for negative inputs are 0, which helps in mitigating the vanishing gradient problem common with other activation functions like sigmoid or tanh.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **softmax function** transforms a vector \\( \\mathbf{z} \\) of real values into a probability distribution over predicted output classes. It's defined by the formula:\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "where:\n",
    "\n",
    "\n",
    "$$\\sigma(\\mathbf{z})_i$$ is the probability of the $$( i )$$-th class, $$( e^{z_i})$$ is the exponential of the $$( i )$$-th input, and $$( \\sum_{k=1}^K e^{z_k} )$$ is the sum of exponentials of all inputs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Understanding Softmax\n",
    "\n",
    "The **softmax function** transforms a list of numbers into probabilities. Here’s how it works:\n",
    "\n",
    "1. **Make Numbers Positive and Exaggerate Differences**: It first calculates the exponential (`e^number`) of each number. This step ensures all numbers are positive and larger numbers stand out more.\n",
    "\n",
    "    Making Numbers Positive and Exaggerating Differences\n",
    "Exponential Function Basics\n",
    "The exponential function $$e^{z_i} \\text{ for each } i$$ is a mathematical function that grows faster as the value of \n",
    "x increases. Here are some key properties of the exponential function that are relevant in the context of the softmax function:\n",
    "\n",
    "Positive Output: For any real number \n",
    "\n",
    "x, \n",
    "\n",
    "\n",
    "e \n",
    "x\n",
    "  is always positive. This property is crucial because it ensures that there are no negative probabilities, which would be nonsensical in the context of probability distributions.\n",
    "\n",
    "Amplifying Differences: The exponential function is an increasing function, which means larger input values will result in exponentially larger output values. This characteristic is particularly useful in highlighting the differences between numbers. Small differences in the input logits can become significant differences in the exponentiated values.\n",
    "\n",
    "Application in Softmax\n",
    "Let's consider a vector of logits, $$\\mathbf{z} = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{bmatrix}$$, representing the raw scores from a model for three classes. Here's what happens when we apply the exponential function:\n",
    "\n",
    "Before Exponentiation: \n",
    "\n",
    "Suppose $$z_1 = 1, \\quad z_2 = 2, \\quad z_3 = 3$$ The differences between each are just 1 unit.\n",
    "\n",
    "After Exponentiation:\n",
    "\n",
    "$$e^{z_1} = e^1 \\approx 2.718, \\quad e^{z_2} = e^2 \\approx 7.389, \\quad e^{z_3} = e^3 \\approx 20.086$$\n",
    "\n",
    "\n",
    "\n",
    "As you can see, after applying the exponential function:\n",
    "\n",
    "All Outputs Are Positive: There are no negative values, fitting the requirement for probabilities.\n",
    "Differences Are Exaggerated: The difference between $$\\(e^{z_1}\\)$$ and $$(e^{z_2})$$ is about 4.671, and between $$(e^{z_2})$$ and $$(e^{z_3})$$ is about 12.697.\n",
    ", compared to the original differences of 1. This exaggeration helps in clearly distinguishing between the logits, especially in scenarios where one class is significantly more likely than the others.\n",
    "Mathematical and Practical Importance\n",
    "This transformation has both mathematical and practical implications:\n",
    "\n",
    "Avoiding Numerical Underflow: By transforming all scores into positive numbers, it guards against issues related to numerical underflow in computers, where very small numbers (close to zero) might be rounded down to zero.\n",
    "Focus on Higher Scores: In classification tasks, you often want the class with the highest logit to have a significantly higher probability after normalization. The exponential function ensures that if one logit is slightly larger than another, its exponential will be much larger, heavily influencing the probability distribution.\n",
    "This step is foundational before moving to normalization, where these exponentiated values are converted into a proper probability distribution that sums to 1, which is covered in the subsequent steps of the softmax function. This ensures that the output of the softmax function reflects both the relative differences and the absolute magnitudes of the input logits in a probabilistically meaningful way.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Add Them Up**: Then, it adds up all these exponentials. This total will help in scaling the numbers down in the next step.\n",
    "\n",
    "3. **Turn Into Probabilities**: Each number’s exponential is then divided by the total from the previous step. This makes each number a probability, with all probabilities adding up to 1.\n",
    "\n",
    "This process is mathematically represented as:\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "Where each $$( \\sigma(\\mathbf{z})_i )$$ is the probability corresponding to each original number after applying the softmax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z1 is going to be the result of the matrix multiplication (dot product, hence .dot) of W1, which has dimensions\n",
    "of 10x784, and X, which is, per the Gradient Descent function call at the bottom of the script, the X_train variable.\n",
    "\n",
    "Since the X_train variable is equal to, starting at 1000, and going for the rest of the samples (m), it will be a matrix\n",
    "with dimensions 784 x m, or 784 x 32,000.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Therefore, the product will be a matrix of dimension 10 x 32,000.\n",
    "\n",
    "We then add, or broadcast the bias vector, b1, to each element in the 10 x 32,000 matrix.\n",
    "\n",
    "\n",
    "We now have Z1.\n",
    "\n",
    "\n",
    "Next, we apply the ReLU function to each element in Z1, which gives us A1 (Still with dimensions 10 x 32,000)\n",
    "\n",
    "\n",
    "We broadcast the b2 vector on each element for the product, which gives us Z2.\n",
    "\n",
    "\n",
    "Finally, we apply the softmax function to Z2.  Referring back to the softmax explanation, for each column \n",
    "in Z2 ( 10 x 32,000), so 32,000 columns, each logit (element in each column) is exponentiated.  Then, we take\n",
    "the sum of the exponentiated logits, and divide each exponentiated logit by that sum.\n",
    "\n",
    "Now, A2 is a 10 x 32,000 matrix, with each column being a probability distribution which will then be compared\n",
    "against the actual labels created by the upcoming one hot function.  This comparision is how the model \n",
    "computes the gradients and updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ReLU_deriv(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding:\n",
    "\n",
    "Per the Gradient Descent function input parameters:\n",
    "\n",
    "```python\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 1000)\n",
    "\n",
    "```\n",
    "\n",
    "We see that with the gradient_descent function call, that Y is the Y_train variable, which is:\n",
    "\n",
    "```python\n",
    "\n",
    "Y_train = data_train[0]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "So, we can see that it is talking about the labels.\n",
    "\n",
    "If we examine the dataset, we will see (before any transposition), that the first column is called \"Label\", and each row therefore in the first [0] column, has the names.  \n",
    "\n",
    "In the dataset used in the video link, the first 5 (before randomization of course), are 1,0,1,4,0.\n",
    "\n",
    "\n",
    "So, one hot encoding basically creates the ground truth matrix that this model will check itself against.  The one hot matrix will have a dimension of 10 x 32,000 and, for example's sake, let's assume that the first 10 columns (remember, we are dealing with a transposed matrix) are 1,0,1,4,0,0,7,3,5:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|ccccccccc}\n",
    "\\text{Class/Example} & 1 & 0 & 1 & 4 & 0 & 0 & 7 & 3 & 5 & 3\\\\\n",
    "\\hline\n",
    "0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\\\\n",
    "4 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "7 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
